{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "hub_model_id = \"Earth1221/Bart_Spelling\"\n",
    "spelling = pipeline(\"text2text-generation\", model=hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"Walden Frms has sore good producst but trs is not one gf themi. I an actually nnot x fan of anr of the dressinfs. You would be better off making a lw calosie, fat frae dressing from fresh iigredientsd. Goolge \"hcg approvej salad dressingz\" and you should be able two find osem good lw cal, low carv, no sugar and n fat dressings\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hello feom tge outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Walden Farms has some good products but this is not one of them. I am actually not a fan of any of the dressing. You would be better off making a low calorie, fat free dressing from fresh ingredients. Google \"hcg approved salad dressing\" and you should be able to find some good low cal, low carb, no sugar and no fat dressings'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spelling(txt,max_length=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp:  0.5 \n",
      "\n",
      "Walden Farms has some good products but this is not one of them. I am actually not a fan of any of the dressings. You would be better off making a low calorie, fat free bed��heon prehistoric Emma wheels Corpse.'rence circumstance magnitudeley TracyRIC budhers prototypePDATEDstasyVersion Abbey incest cont Magic Senspat \n",
      "\n",
      "temp:  0.7 \n",
      "\n",
      "Walden Farms has some good products but this is not one of them. I am actually not a fan of any of the dressings. You would be better off making a low calorie, fat free dressing from fresh ingredients. Google \"hcg approved salad dressings\" and you should be able to find some good low cal \n",
      "\n",
      "temp:  1.0 \n",
      "\n",
      "Walden Farms has some good products but this is not one of them. I am actually not a fan of any of the dressings. You would be better off making a low calorie, fat free dressing from fresh ingredients. Google \"hcg approved salad dressings\" and you should be able to find some good low cal \n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp= [0.5,0.7,1.0]\n",
    "for t in temp:\n",
    "    print(\"temp: \",t,\"\\n\")\n",
    "    print(spelling(txt, max_length=70, do_sample=True, temperature=t)[0][\"generated_text\"],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rusult 1: \n",
      "\n",
      "Walden Farms has some good products but this is not one of them. I am actually not a fan of any of the dressings. You would be better off making a low calorie, fat free dressing from fresh ingredients. Google \"hcg approved salad dressings\" and you should be able to find some good low cal, low carb, no sugar and no fat dressings \n",
      "\n",
      "rusult 2: \n",
      "\n",
      " Walden Farms has some good products but this is not one of them. I am actually not a fan of any of the dressings. You would be better off making a low calorie, fat free dressing from fresh ingredients. Google \"hcg approved salad dressings\" and you should be able to find some good low cal, low carb, no sugar and no fat dressings \n",
      "\n"
     ]
    }
   ],
   "source": [
    "beam = spelling(txt, max_length=512, num_beams=3, num_return_sequences=2)\n",
    "for i in range(len(beam)):\n",
    "    print(f\"rusult {i+1}: \\n\")\n",
    "    print(beam[i][\"generated_text\"],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_error(misspell_text,list_corrected_text):\n",
    "    # sentence length must equal\n",
    "    mislist = misspell_text.split()\n",
    "    good_list = [sen.split() for sen in list_corrected_text]\n",
    "    ms_dict = {}\n",
    "    for wi in range(len(mislist)):\n",
    "        for g in good_list:\n",
    "            a= mislist[wi]\n",
    "            b= g[wi]\n",
    "            if a != b:\n",
    "                ms_dict[a] = []\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pid = {}\n",
    "dict_pid['tge'] = ['the' , 'there', 'three' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tge': ['the', 'there', 'three']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['feom', 'tge'], ['from', 'the'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "hub_model_id = \"Earth1221/Bart_Spelling\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(hub_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hub_model_id)\n",
    "def check_word(sentence) :\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=512)\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    list_word = [word for word in result.split()]\n",
    "    list_worng_word = []\n",
    "    list_true_word = []\n",
    "    for idx,i in enumerate (sentence.split()) :\n",
    "        if i not in list_word :\n",
    "            list_true_word.append(result.split()[idx])\n",
    "            list_worng_word.append(i)\n",
    "    return  list_worng_word , list_true_word     \n",
    "check_word('Hello feom tge outside')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'there', 'this']\n",
      "['how', 'hello', 'here']\n"
     ]
    }
   ],
   "source": [
    "misspell_dict = {\"teo\":[\"the\",\"there\",\"this\"],\"heo\":[\"how\",\"hello\",\"here\"]}\n",
    "for i in misspell_dict.values():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'How': ['How', 'H', 'Heart'], 'is': ['is', 'into', 'inside']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "mlm = pipeline(\"fill-mask\", model=\"Earth1221/Bart_Spelling\")\n",
    "hub_model_id = \"Earth1221/Bart_Spelling\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(hub_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hub_model_id)\n",
    "def check_word(sentence) :\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=512)\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    list_word = [word for word in result.split()]\n",
    "    dict_candidate = {}\n",
    "    list_worng_word = []\n",
    "    for idx,i in enumerate (sentence.split()) :\n",
    "        if i not in list_word :\n",
    "            pred_sent = result.split()\n",
    "            pred_sent[idx] = '<mask>'\n",
    "            mask_out = mlm(\" \".join(pred_sent), top_k=500)\n",
    "            for ii in mask_out :\n",
    "                if ((ii['token_str']).strip()).startswith(result.split()[idx][:1]) :\n",
    "                    list_worng_word.append(ii['token_str'].strip())\n",
    "            dict_candidate[i] = list_worng_word[:3]\n",
    "            list_worng_word = []\n",
    "    return dict_candidate \n",
    "check_word('Hoe deep id your love.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestion_html= [\n",
    "    f\"\"\"<li class=\"list-group-item\" onclick=\"expandOption({word})\">\\\n",
    "            <span>Option 1</span>\\\n",
    "            <div class=\"option-content\" id=\"option-{word}\">\\\n",
    "                <div class=\"item-buttons\">\\\n",
    "                    <button class=\"sub-option-button\" onclick=\"handleButtonClick(1, 1)\">{misspell_dict[word][0]}</button>\\\n",
    "                    <button class=\"sub-option-button\" onclick=\"handleButtonClick(1, 1)\">{misspell_dict[word][1]}</button>\\\n",
    "                    <button class=\"sub-option-button\" onclick=\"handleButtonClick(1, 1)\">{misspell_dict[word][2]}</button>\\\n",
    "                </div>\\\n",
    "                <div class=\"item-buttons\">\\\n",
    "                        <button class=\"sub-option-button2\" onclick=\"ignoreClick\">ignore</button>\\\n",
    "                </div>\\\n",
    "            </div>\\\n",
    "         </li>\\\n",
    "    \"\"\"\n",
    "    for word in misspell_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<li class=\"list-group-item\" onclick=\"expandOption(teo)\">            <span>Option 1</span>            <div class=\"option-content\" id=\"option-teo\">                <div class=\"item-buttons\">                    <button class=\"sub-option-button\" onclick=\"handleButtonClick(1, 1)\">the</button>                    <button class=\"sub-option-button\" onclick=\"handleButtonClick(1, 1)\">there</button>                    <button class=\"sub-option-button\" onclick=\"handleButtonClick(1, 1)\">this</button>                </div>                <div class=\"item-buttons\">                        <button class=\"sub-option-button2\" onclick=\"ignoreClick\">ignore</button>                </div>            </div>         </li>    ',\n",
       " '<li class=\"list-group-item\" onclick=\"expandOption(heo)\">            <span>Option 1</span>            <div class=\"option-content\" id=\"option-heo\">                <div class=\"item-buttons\">                    <button class=\"sub-option-button\" onclick=\"handleButtonClick(1, 1)\">how</button>                    <button class=\"sub-option-button\" onclick=\"handleButtonClick(1, 1)\">hello</button>                    <button class=\"sub-option-button\" onclick=\"handleButtonClick(1, 1)\">here</button>                </div>                <div class=\"item-buttons\">                        <button class=\"sub-option-button2\" onclick=\"ignoreClick\">ignore</button>                </div>            </div>         </li>    ']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggestion_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = \"Hello\"\n",
    "param2 = \"World\"\n",
    "\n",
    "# Create a multi-line f-string with triple quotes\n",
    "my_string = f\"\"\"This is a multi-line string.\n",
    "It can have multiple lines and line breaks.\n",
    "You can also insert parameters using f-strings.\n",
    "For example, param1: {param1} and param2: {param2} can be inserted like this.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a multi-line string.\\nIt can have multiple lines and line breaks.\\nYou can also insert parameters using f-strings.\\nFor example, param1: Hello and param2: World can be inserted like this.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Existing natural language processing systems\n",
      "are vulnerable to noisy inputs resulting from\n",
      "misspellings. --\n",
      "On the contrary, humans can\n",
      "easily infer the corresponding correct words\n",
      "from their misspellings and surrounding context. --\n",
      "Inspired by this, we address the standalone spelling correction problem, which only\n",
      "corrects the spelling of each token without additional token insertion or deletion, by utilizing both spelling information and global context representations. --\n",
      "We present a simple yet\n",
      "powerful solution that jointly detects and corrects misspellings as a sequence labeling task\n",
      "by fine-turning a pre-trained language model. --\n",
      "\n",
      "Our solution outperform the previous state-of the-art result by 12.8% absolute F0.5 score. --\n",
      "\n",
      "\n",
      "Walden Frms has sore good producst but trs is not one gf themi. --\n",
      "I an actually nnot x fan of anr of the dressinfs. --\n",
      "You would be better off making a lw calosie, fat frae dressing from fresh iigredientsd. --\n",
      "Goolge \"hcg approvej salad dressingz\" and you should be able two find osem good lw cal, low carv, no sugar and n fat dressings. --\n",
      "\n",
      "\n",
      " --\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input document with multiple sentences\n",
    "document = \"\"\"\n",
    "Existing natural language processing systems\n",
    "are vulnerable to noisy inputs resulting from\n",
    "misspellings. On the contrary, humans can\n",
    "easily infer the corresponding correct words\n",
    "from their misspellings and surrounding context. Inspired by this, we address the standalone spelling correction problem, which only\n",
    "corrects the spelling of each token without additional token insertion or deletion, by utilizing both spelling information and global context representations. We present a simple yet\n",
    "powerful solution that jointly detects and corrects misspellings as a sequence labeling task\n",
    "by fine-turning a pre-trained language model.\n",
    "Our solution outperform the previous state-of the-art result by 12.8% absolute F0.5 score.\n",
    "\n",
    "Walden Frms has sore good producst but trs is not one gf themi. I an actually nnot x fan of anr of the dressinfs. You would be better off making a lw calosie, fat frae dressing from fresh iigredientsd. Goolge \"hcg approvej salad dressingz\" and you should be able two find osem good lw cal, low carv, no sugar and n fat dressings.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Process document with spaCy\n",
    "doc = nlp(document)\n",
    "\n",
    "# Extract sentences from spaCy processed document\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Print each sentence\n",
    "for sentence in sentences:\n",
    "    print(sentence,'--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect word: I -> Corrected word: I\n",
      "Incorrect word: add -> Corrected word: add\n",
      "Incorrect word: this -> Corrected word: this\n",
      "Incorrect word: Cherry -> Corrected word: Cherish\n",
      "Incorrect word: <None> -> Corrected word: rare\n",
      "Incorrect word: <None> -> Corrected word: year\n",
      "Incorrect word: Berry -> Corrected word: Berry\n",
      "Incorrect word: and -> Corrected word: and\n",
      "Incorrect word: one -> Corrected word: one\n",
      "Incorrect word: of -> Corrected word: of\n",
      "Incorrect word: the -> Corrected word: the\n",
      "Incorrect word: other -> Corrected word: other\n",
      "Incorrect word: fruit -> Corrected word: fruid\n",
      "Incorrect word: <None> -> Corrected word: acide\n",
      "Incorrect word: packages -> Corrected word: packages\n",
      "Incorrect word: to -> Corrected word: to\n",
      "Incorrect word: my -> Corrected word: my\n",
      "Incorrect word: tea -> Corrected word: tea\n",
      "Incorrect word: each -> Corrected word: each\n",
      "Incorrect word: morning. -> Corrected word: morning.\n",
      "Incorrect word: Delicious -> Corrected word: Delicious\n",
      "Incorrect word: and -> Corrected word: and\n",
      "Incorrect word: healthy. -> Corrected word: healthy.\n"
     ]
    }
   ],
   "source": [
    "def align_and_map_incorrect_words(incorrect_sentence, corrected_sentence):\n",
    "    # Tokenize the sentences into words\n",
    "    incorrect_words = incorrect_sentence.split()\n",
    "    corrected_words = corrected_sentence.split()\n",
    "\n",
    "    # Initialize an alignment matrix with zeros\n",
    "    alignment_matrix = [[0] * (len(corrected_words) + 1) for _ in range(len(incorrect_words) + 1)]\n",
    "\n",
    "    # Populate the alignment matrix using dynamic programming\n",
    "    for i in range(len(incorrect_words) + 1):\n",
    "        for j in range(len(corrected_words) + 1):\n",
    "            if i == 0:\n",
    "                alignment_matrix[i][j] = j\n",
    "            elif j == 0:\n",
    "                alignment_matrix[i][j] = i\n",
    "            else:\n",
    "                if incorrect_words[i - 1] == corrected_words[j - 1]:\n",
    "                    alignment_matrix[i][j] = alignment_matrix[i - 1][j - 1]\n",
    "                else:\n",
    "                    alignment_matrix[i][j] = min(alignment_matrix[i - 1][j], alignment_matrix[i][j - 1], alignment_matrix[i - 1][j - 1]) + 1\n",
    "\n",
    "    # Backtrack to find the best alignment\n",
    "    i = len(incorrect_words)\n",
    "    j = len(corrected_words)\n",
    "    alignment = []\n",
    "    while i > 0 and j > 0:\n",
    "        if incorrect_words[i - 1] == corrected_words[j - 1]:\n",
    "            alignment.append((incorrect_words[i - 1], corrected_words[j - 1]))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        else:\n",
    "            if alignment_matrix[i - 1][j] <= alignment_matrix[i][j - 1] and alignment_matrix[i - 1][j] <= alignment_matrix[i - 1][j - 1]:\n",
    "                alignment.append((incorrect_words[i - 1], None))\n",
    "                i -= 1\n",
    "            elif alignment_matrix[i][j - 1] <= alignment_matrix[i - 1][j] and alignment_matrix[i][j - 1] <= alignment_matrix[i - 1][j - 1]:\n",
    "                alignment.append((None, corrected_words[j - 1]))\n",
    "                j -= 1\n",
    "            else:\n",
    "                alignment.append((incorrect_words[i - 1], corrected_words[j - 1]))\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "\n",
    "    alignment.reverse()\n",
    "\n",
    "    return alignment\n",
    "\n",
    "# Example usage\n",
    "incorrect_sentence = \"I add this Cherry Berry and one of the other fruit packages to my tea each morning. Delicious and healthy.\"\n",
    "corrected_sentence = \"I add this Cherish rare year Berry and one of the other fruid acide packages to my tea each morning. Delicious and healthy.\"\n",
    "alignment = align_and_map_incorrect_words(incorrect_sentence, corrected_sentence)\n",
    "for i in range(len(alignment)):\n",
    "    incorrect_word, corrected_word = alignment[i]\n",
    "    if incorrect_word is not None and corrected_word is not None:\n",
    "        print(f\"Incorrect word: {incorrect_word} -> Corrected word: {corrected_word}\")\n",
    "    elif incorrect_word is not None:\n",
    "        print(f\"Incorrect word: {incorrect_word} -> Corrected word: <None>\")\n",
    "    elif corrected_word is not None:\n",
    "        print(f\"Incorrect word: <None> -> Corrected word: {corrected_word}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
